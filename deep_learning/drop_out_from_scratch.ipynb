{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd, gluon\n",
    "mx.random.seed(1)\n",
    "ctx = mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()\n",
    "\n",
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/255, label.astype(np.float32)\n",
    "\n",
    "def relu(X):\n",
    "    return nd.maximum(X, 0)\n",
    "\n",
    "def dropout(X, drop_probability):\n",
    "    keep_probability = 1 - drop_probability\n",
    "    mask = nd.random_uniform(0, 1.0, X.shape, ctx=X.context) < keep_probability\n",
    "    #############################\n",
    "    #  Avoid division by 0 when scaling\n",
    "    #############################\n",
    "    if keep_probability > 0.0:\n",
    "        scale = (1/keep_probability)\n",
    "    else:\n",
    "        scale = 0.0\n",
    "    return mask * X * scale\n",
    "\n",
    "def softmax(y_linear):\n",
    "    exp = nd.exp(y_linear-nd.max(y_linear))\n",
    "    partition = nd.nansum(exp, axis=0, exclude=True).reshape((-1,1))\n",
    "    return exp / partition\n",
    "\n",
    "def softmax_cross_entropy(yhat_linear, y):\n",
    "    return - nd.nansum(y * nd.log_softmax(yhat_linear), axis=0, exclude=True)\n",
    "\n",
    "def net(X, drop_prob=0.0):\n",
    "    #######################\n",
    "    #  Compute the first hidden layer\n",
    "    #######################\n",
    "    h1_linear = nd.dot(X, W1) + b1\n",
    "    h1 = relu(h1_linear)\n",
    "    h1 = dropout(h1, drop_prob)\n",
    "\n",
    "    #######################\n",
    "    #  Compute the second hidden layer\n",
    "    #######################\n",
    "    h2_linear = nd.dot(h1, W2) + b2\n",
    "    h2 = relu(h2_linear)\n",
    "    h2 = dropout(h2, drop_prob)\n",
    "\n",
    "    #######################\n",
    "    #  Compute the output layer.\n",
    "    #  We will omit the softmax function here\n",
    "    #  because it will be applied\n",
    "    #  in the softmax_cross_entropy loss\n",
    "    #######################\n",
    "    yhat_linear = nd.dot(h2, W3) + b3\n",
    "    return yhat_linear\n",
    "\n",
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad\n",
    "        \n",
    "\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    numerator = 0.\n",
    "    denominator = 0.\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx).reshape((-1,784))\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        numerator += nd.sum(predictions == label)\n",
    "        denominator += data.shape[0]\n",
    "    return (numerator / denominator).asscalar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mnist = mx.test_utils.get_mnist()\n",
    "batch_size = 64\n",
    "\n",
    "train_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=True, transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                                     batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = nd.random_normal(shape=(784,256), ctx=ctx) *.01\n",
    "b1 = nd.random_normal(shape=256, ctx=ctx) * .01\n",
    "\n",
    "W2 = nd.random_normal(shape=(256,128), ctx=ctx) *.01\n",
    "b2 = nd.random_normal(shape=128, ctx=ctx) * .01\n",
    "\n",
    "W3 = nd.random_normal(shape=(128,10), ctx=ctx) *.01\n",
    "b3 = nd.random_normal(shape=10, ctx=ctx) *.01\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(epochs, moving_loss, learning_rate, drop_prob=0.0):\n",
    "    for e in range(epochs):\n",
    "        for i, (data, label) in enumerate(train_data):\n",
    "            data = data.as_in_context(ctx).reshape((-1,784))\n",
    "            label = label.as_in_context(ctx)\n",
    "            label_one_hot = nd.one_hot(label, 10)\n",
    "            with autograd.record():\n",
    "                ################################\n",
    "                #   Drop out 50% of hidden activations on the forward pass\n",
    "                ################################\n",
    "                output = net(data, drop_prob=drop_prob)\n",
    "                loss = softmax_cross_entropy(output, label_one_hot)\n",
    "            loss.backward()\n",
    "            SGD(params, learning_rate)\n",
    "\n",
    "            ##########################\n",
    "            #  Keep a moving average of the losses\n",
    "            ##########################\n",
    "            if i == 0:\n",
    "                moving_loss = nd.mean(loss).asscalar()\n",
    "            else:\n",
    "                moving_loss = .99 * moving_loss + .01 * nd.mean(loss).asscalar()\n",
    "\n",
    "        test_accuracy = evaluate_accuracy(test_data, net)\n",
    "        train_accuracy = evaluate_accuracy(train_data, net)\n",
    "        print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.07306404552387685, Train_acc 0.98628336, Test_acc 0.9771\n",
      "Epoch 1. Loss: 0.06342638775173977, Train_acc 0.9884833, Test_acc 0.9776\n",
      "Epoch 2. Loss: 0.05340114151046477, Train_acc 0.9892, Test_acc 0.979\n",
      "Epoch 3. Loss: 0.049753681222384195, Train_acc 0.9913333, Test_acc 0.9793\n",
      "Epoch 4. Loss: 0.04727674141073009, Train_acc 0.99205, Test_acc 0.9799\n",
      "Epoch 5. Loss: 0.044759081296268315, Train_acc 0.9926, Test_acc 0.9806\n",
      "Epoch 6. Loss: 0.04101160041203503, Train_acc 0.99413335, Test_acc 0.9807\n",
      "Epoch 7. Loss: 0.04085139174864844, Train_acc 0.9949833, Test_acc 0.981\n",
      "Epoch 8. Loss: 0.037974419814203124, Train_acc 0.99521667, Test_acc 0.9808\n",
      "Epoch 9. Loss: 0.03294160778695116, Train_acc 0.9952833, Test_acc 0.9808\n",
      "########\n",
      "Epoch 0. Loss: 0.10964125070984783, Train_acc 0.9920167, Test_acc 0.9795\n",
      "Epoch 1. Loss: 0.09674480023945269, Train_acc 0.9920833, Test_acc 0.9803\n",
      "Epoch 2. Loss: 0.09693814251609556, Train_acc 0.99161667, Test_acc 0.9786\n",
      "Epoch 3. Loss: 0.09040206071322242, Train_acc 0.99255, Test_acc 0.9804\n",
      "Epoch 4. Loss: 0.08518060830782473, Train_acc 0.99275, Test_acc 0.98\n",
      "Epoch 5. Loss: 0.08750079655763134, Train_acc 0.9925, Test_acc 0.9808\n",
      "Epoch 6. Loss: 0.08051821650747044, Train_acc 0.99228334, Test_acc 0.9803\n",
      "Epoch 7. Loss: 0.08504159045918934, Train_acc 0.9937, Test_acc 0.9807\n",
      "Epoch 8. Loss: 0.07566620818420533, Train_acc 0.99366665, Test_acc 0.98\n",
      "Epoch 9. Loss: 0.08053815377801644, Train_acc 0.99385, Test_acc 0.981\n",
      "########\n",
      "Epoch 0. Loss: 0.4978580605644937, Train_acc 0.97385, Test_acc 0.9679\n",
      "Epoch 1. Loss: 0.4331249808654699, Train_acc 0.973, Test_acc 0.9666\n",
      "Epoch 2. Loss: 0.4403287489190183, Train_acc 0.97186667, Test_acc 0.9669\n",
      "Epoch 3. Loss: 0.4151173273283808, Train_acc 0.9720167, Test_acc 0.9676\n",
      "Epoch 4. Loss: 0.39284266322646094, Train_acc 0.9719167, Test_acc 0.9665\n",
      "Epoch 5. Loss: 0.4082924969557893, Train_acc 0.97095, Test_acc 0.9658\n",
      "Epoch 6. Loss: 0.400436454183991, Train_acc 0.97173333, Test_acc 0.9646\n",
      "Epoch 7. Loss: 0.3822517660927449, Train_acc 0.97148335, Test_acc 0.965\n",
      "Epoch 8. Loss: 0.3788843446958897, Train_acc 0.97078335, Test_acc 0.9649\n",
      "Epoch 9. Loss: 0.3716901034308196, Train_acc 0.97136664, Test_acc 0.9655\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "moving_loss = 0.\n",
    "learning_rate = .001\n",
    "drop_prob_1 = .5\n",
    "drop_prob_0 = .2\n",
    "drop_prob_2 = .8\n",
    "\n",
    "run_model(epochs, moving_loss, learning_rate, drop_prob_0)\n",
    "print(\"########\")\n",
    "run_model(epochs, moving_loss, learning_rate, drop_prob_1)\n",
    "print(\"########\")\n",
    "run_model(epochs, moving_loss, learning_rate, drop_prob_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.23804695222742336, Train_acc 0.9492, Test_acc 0.9473\n",
      "Epoch 1. Loss: 0.15733224429679663, Train_acc 0.96648335, Test_acc 0.9638\n",
      "Epoch 2. Loss: 0.1288373542143002, Train_acc 0.97361666, Test_acc 0.9682\n",
      "Epoch 3. Loss: 0.1081733464996873, Train_acc 0.97978336, Test_acc 0.9729\n",
      "Epoch 4. Loss: 0.0872821198329437, Train_acc 0.9798167, Test_acc 0.9722\n",
      "Epoch 5. Loss: 0.07667797150629169, Train_acc 0.98606664, Test_acc 0.977\n",
      "Epoch 6. Loss: 0.07144925333083696, Train_acc 0.98861665, Test_acc 0.9778\n",
      "Epoch 7. Loss: 0.06285207139919195, Train_acc 0.98866665, Test_acc 0.9778\n",
      "Epoch 8. Loss: 0.05466447627110476, Train_acc 0.99111664, Test_acc 0.9784\n",
      "Epoch 9. Loss: 0.05116152321979755, Train_acc 0.99275, Test_acc 0.9799\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-5b7d8d77ae0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mnet1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model_gluon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpoints\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"net1.params\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;31m# print(\"#########\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# run_model_gluon(num_hidden, drop_prob=.5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#Build with gluon\n",
    "\n",
    "num_hidden = 256\n",
    "num_output = 10\n",
    "num_input = 784\n",
    "drop_prob = .5\n",
    "epochs = 10\n",
    "smoothing_constant = .01\n",
    "learning_rate = .1\n",
    "\n",
    "net = gluon.nn.Sequential()\n",
    "\n",
    "def run_model_gluon(num_hidden, drop_prob, learning_rate=.1, epochs = 10, smoothing_constants = .1):\n",
    "    with net.name_scope():\n",
    "        net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "        net.add(gluon.nn.Dropout(drop_prob))\n",
    "        net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "        net.add(gluon.nn.Dropout(drop_prob))\n",
    "        net.add(gluon.nn.Dense(num_output))\n",
    "    \n",
    "    net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n",
    "\n",
    "    softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': learning_rate})\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i, (data, label) in enumerate(train_data):\n",
    "            data = data.as_in_context(ctx).reshape((-1, 784))\n",
    "            label = label.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "                loss.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "\n",
    "            ##########################\n",
    "            #  Keep a moving average of the losses\n",
    "            ##########################\n",
    "            curr_loss = nd.mean(loss).asscalar()\n",
    "            moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
    "                           else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "        test_accuracy = evaluate_accuracy(test_data, net)\n",
    "        train_accuracy = evaluate_accuracy(train_data, net)\n",
    "        print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "              (e, moving_loss, train_accuracy, test_accuracy))\n",
    "    \n",
    "    return net\n",
    "        \n",
    "def save_model(net, output_path):\n",
    "    net.save_parameters(output_path)\n",
    "    \n",
    "net1 = run_model_gluon(num_hidden, drop_prob=.2)\n",
    "save_model(net1, os.path.join(\"checkpoints\", \"net1.params\"))\n",
    "# print(\"#########\")\n",
    "# run_model_gluon(num_hidden, drop_prob=.5)\n",
    "# print(\"#########\")\n",
    "# run_model_gluon(num_hidden, drop_prob=.8)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
